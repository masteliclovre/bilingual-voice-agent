"""FastAPI server that runs the bilingual voice agent pipeline on a GPU backend.

Run this on any GPU host and point REMOTE_AGENT_URL from the
voice_agent.py client to this server. The server keeps per-session memory so the
conversation stays coherent across turns. Audio is exchanged as 16 kHz mono WAV
(PCM16) payloads to keep latency low.
"""

import numpy as np
import base64
import io
import os
import threading
import uuid
import shutil
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, Optional

from dotenv import load_dotenv
from fastapi import Depends, FastAPI, File, Form, HTTPException, UploadFile, Header
from fastapi.middleware.cors import CORSMiddleware
from faster_whisper import WhisperModel
from openai import OpenAI
from elevenlabs import ElevenLabs

load_dotenv()

# =========================
# Config (mirrors voice_agent.py)
# =========================

# LLM Provider configuration
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "groq").lower()

# API Keys
DEFAULT_GROQ_API_KEY = os.getenv("GROQ_API_KEY", "").strip()
DEFAULT_OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "").strip()

ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY", "")
ELEVENLABS_VOICE_ID = os.getenv("ELEVENLABS_VOICE_ID", "vFQACl5nAIV0owAavYxE")

WHISPER_MODEL = os.getenv("WHISPER_MODEL", "GoranS/whisper-base-1m.hr-ctranslate2")
WHISPER_DEVICE = os.getenv("WHISPER_DEVICE", "cpu")
WHISPER_COMPUTE = os.getenv("WHISPER_COMPUTE", "int8")

# LLM Model configuration
if LLM_PROVIDER == "groq":
    LLM_MODEL = os.getenv("GROQ_MODEL", "llama-3.3-70b-versatile")
else:
    LLM_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

LLM_TEMPERATURE = float(os.getenv("OPENAI_TEMPERATURE", "0.3"))
LLM_MAX_TOKENS = int(os.getenv("OPENAI_MAX_TOKENS", "150"))

MAX_TURNS_IN_WINDOW = int(os.getenv("MAX_TURNS_IN_WINDOW", "8"))
SUMMARY_UPDATE_EVERY = int(os.getenv("SUMMARY_UPDATE_EVERY", "8"))

TARGET_SR = 16000

SERVER_AUTH_TOKEN = os.getenv("REMOTE_SERVER_AUTH_TOKEN", "").strip() or None

app = FastAPI(title="Bilingual Voice Agent GPU Server")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# =========================
# Model Download with LFS Support
# =========================


def find_model_files(base_dir: str) -> Optional[str]:
    """
    Recursively search for model.bin in downloaded directory.
    HuggingFace sometimes nests files in subdirectories.
    Returns the directory containing model.bin, or None.
    """
    base_path = Path(base_dir)
    
    # Search for model.bin recursively
    for model_bin in base_path.rglob("model.bin"):
        model_dir = model_bin.parent
        print(f"   âœ“ Found model.bin in: {model_dir}")
        
        # Check if it's a valid size
        size_mb = model_bin.stat().st_size / (1024 * 1024)
        print(f"   âœ“ model.bin size: {size_mb:.1f} MB")
        
        if size_mb < 10:
            print(f"   âš ï¸  Warning: model.bin is only {size_mb:.2f} MB")
            continue
        
        return str(model_dir)
    
    return None


def download_hf_model_with_lfs(repo_id: str, local_dir: str = "/tmp/whisper_model") -> str:
    """
    Download HuggingFace model ensuring LFS files are properly downloaded.
    Returns the local path to the model directory containing model.bin.
    """
    from huggingface_hub import snapshot_download
    
    print(f"ðŸ“¦ Downloading model from HuggingFace: {repo_id}")
    
    try:
        # Use snapshot_download which handles nested directories properly
        downloaded_path = snapshot_download(
            repo_id=repo_id,
            cache_dir=os.path.dirname(local_dir),
            local_dir=local_dir,
            local_dir_use_symlinks=False,
            resume_download=True,
        )
        
        print(f"   âœ“ Download complete: {downloaded_path}")
        
        # Find where model.bin actually is
        model_dir = find_model_files(downloaded_path)
        
        if not model_dir:
            # List what we actually got
            print(f"\nâŒ model.bin not found. Directory contents:")
            for root, dirs, files in os.walk(downloaded_path):
                level = root.replace(downloaded_path, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f"{indent}{os.path.basename(root)}/")
                subindent = ' ' * 2 * (level + 1)
                for file in files:
                    file_path = os.path.join(root, file)
                    size = os.path.getsize(file_path)
                    print(f"{subindent}{file} ({size} bytes)")
            
            raise RuntimeError(
                f"âŒ model.bin not found in {downloaded_path}\n"
                "The HuggingFace repository might not contain a valid CTranslate2 model.\n"
                "Please verify the model format on HuggingFace."
            )
        
        print(f"âœ… Model ready at: {model_dir}\n")
        return model_dir
        
    except Exception as e:
        print(f"\nâŒ Failed to download model: {e}")
        raise


def load_whisper():
    """Load Whisper model with automatic HuggingFace download support."""
    
    # Check if WHISPER_MODEL is a local path or HF repo
    if os.path.exists(WHISPER_MODEL):
        # Local path - check if model.bin is directly there or in subdirectory
        model_path = WHISPER_MODEL
        print(f"ðŸ“‚ Using local model path: {model_path}")
        
        # Check if we need to search for model.bin in subdirectories
        if not os.path.exists(os.path.join(model_path, "model.bin")):
            print(f"   model.bin not in root, searching subdirectories...")
            found_path = find_model_files(model_path)
            if found_path:
                model_path = found_path
            else:
                raise RuntimeError(f"model.bin not found in {model_path} or subdirectories")
        
    elif "/" in WHISPER_MODEL:
        # HuggingFace repo (format: username/repo-name)
        print(f"ðŸŒ Model is HuggingFace repository: {WHISPER_MODEL}")
        
        # Use a persistent cache directory
        cache_base = os.path.expanduser("~/.cache/whisper_models")
        safe_name = WHISPER_MODEL.replace("/", "_")
        cache_dir = os.path.join(cache_base, safe_name)
        
        # Check if already downloaded and valid
        if os.path.exists(cache_dir):
            existing_model = find_model_files(cache_dir)
            if existing_model:
                model_bin = os.path.join(existing_model, "model.bin")
                size_mb = os.path.getsize(model_bin) / (1024 * 1024)
                print(f"âœ… Using cached model ({size_mb:.1f} MB): {existing_model}")
                model_path = existing_model
            else:
                print(f"âš ï¸  Cached model invalid, re-downloading...")
                shutil.rmtree(cache_dir, ignore_errors=True)
                model_path = download_hf_model_with_lfs(WHISPER_MODEL, cache_dir)
        else:
            # Download from HuggingFace
            model_path = download_hf_model_with_lfs(WHISPER_MODEL, cache_dir)
    else:
        # Assume it's a built-in model name
        model_path = WHISPER_MODEL
        print(f"ðŸ”§ Using built-in model identifier: {model_path}")
    
    # Load the model
    print(f"ðŸš€ Loading Whisper model from: {model_path}")
    kwargs = dict(
        device=WHISPER_DEVICE,
        compute_type=WHISPER_COMPUTE,
        cpu_threads=os.cpu_count(),
        num_workers=1,
    )
    
    try:
        model = WhisperModel(model_path, **kwargs)
        print(f"âœ… Whisper model loaded successfully")
    except Exception as e:
        print(f"âŒ Failed to load Whisper model: {e}")
        print(f"\nTroubleshooting:")
        print(f"1. Check if model files exist: ls -lh {model_path}")
        print(f"2. Verify model.bin size: du -h {model_path}/model.bin")
        print(f"3. Try re-downloading by deleting: rm -rf {cache_base}/{safe_name if '/' in WHISPER_MODEL else ''}")
        raise
    
    # GPU warmup
    if WHISPER_DEVICE == "cuda":
        print("ðŸ”¥ Warming up GPU...")
        dummy = np.zeros(16000, dtype=np.float32)
        try:
            list(model.transcribe(dummy, beam_size=1, vad_filter=False))
            print("âœ… GPU warmup complete")
        except Exception as e:
            print(f"âš ï¸  GPU warmup warning: {e}")
    
    return model


# =========================
# Utilities shared with voice_agent
# =========================


def resample_to_16k(audio_np: np.ndarray, src_sr: int) -> np.ndarray:
    if src_sr == TARGET_SR:
        return audio_np
    target_len = int(len(audio_np) * TARGET_SR / src_sr)
    if target_len <= 0:
        return np.zeros(1, dtype=np.float32)
    from scipy.signal import resample

    return resample(audio_np, target_len).astype(np.float32)


def whisper_transcribe(whisper: WhisperModel, wav_buf: io.BytesIO):
    if wav_buf.getbuffer().nbytes < 32000:
        return "", None
    import wave

    wav_buf.seek(0)
    with wave.open(wav_buf, 'rb') as wf:
        sr = wf.getframerate()
        nframes = wf.getnframes()
        pcm = wf.readframes(nframes)
    audio = np.frombuffer(pcm, dtype=np.int16).astype(np.float32) / 32768.0
    if sr != TARGET_SR:
        audio = resample_to_16k(audio, sr)
    
    segments, info = whisper.transcribe(
        audio=audio,
        beam_size=1,
        vad_filter=True,
        temperature=0.0,
        language=None,
        condition_on_previous_text=False,
        word_timestamps=False,
        without_timestamps=True,
    )
    text = "".join(seg.text for seg in segments).strip()
    lang = getattr(info, "language", None)
    return text, lang


llm_clients: Dict[str, "LLMClient"] = {}


class LLMClient:
    def __init__(self, api_key: str, provider: str = LLM_PROVIDER):
        api_key = api_key.strip()
        if not api_key:
            raise RuntimeError(f"Missing API key for {provider} LLM provider.")
        
        self.api_key = api_key
        self.provider = provider
        self.model = LLM_MODEL
        
        # Configure client based on provider
        if provider == "groq":
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://api.groq.com/openai/v1"
            )
        else:  # openai
            self.client = OpenAI(api_key=api_key)

    def complete(self, messages, temperature=LLM_TEMPERATURE, max_tokens=LLM_MAX_TOKENS):
        params = dict(
            model=self.model,
            messages=messages,
            temperature=temperature,
        )
        if max_tokens:
            params["max_tokens"] = max_tokens
        resp = self.client.chat.completions.create(**params)
        return resp.choices[0].message.content.strip()


class Memory:
    def __init__(self, llm: LLMClient):
        self.llm = llm
        self.summary = ""
        self.window = []
        self.user_turns_since_summary = 0

    def add_user(self, content: str):
        self.window.append({"role": "user", "content": content})
        self.user_turns_since_summary += 1
        self._trim_window()

    def add_assistant(self, content: str):
        self.window.append({"role": "assistant", "content": content})
        self._trim_window()

    def _trim_window(self):
        if len(self.window) > MAX_TURNS_IN_WINDOW * 2:
            self.window = self.window[-MAX_TURNS_IN_WINDOW * 2:]

    def maybe_summarize(self):
        if self.user_turns_since_summary < SUMMARY_UPDATE_EVERY:
            return
        sys_prompt = (
            "You are a memory compressor. Summarize the following conversation into concise bullet points "
            "capturing user preferences, facts, goals, and unresolved tasks. Keep neutral tone. Max ~150 words."
        )
        msgs = [{"role": "system", "content": sys_prompt}]
        if self.summary:
            msgs.append({"role": "system", "content": f"Existing summary memory:\n{self.summary}"})
        msgs.extend(self.window)
        try:
            new_summary = self.llm.complete(msgs, temperature=0.2)
            self.summary = new_summary
            self.user_turns_since_summary = 0
        except Exception as exc:
            print("Memory summarize error:", exc)

    def build_prompt(self, user_lang_hint: Optional[str]):
        system = (
            "You are a concise, helpful bilingual assistant for Croatian and English. Detect the user's language "
            "and ALWAYS reply in that language. If the user mixes languages, keep their dominant language. "
            "Keep answers short for voice (2â€“5 sentences). Be context-aware and remember prior details from the summary."
        )
        if (user_lang_hint or "").startswith("hr"):
            system += " Prefer Croatian if the user speaks Croatian."
        else:
            system += " Prefer English if the user speaks English."
        msgs = [{"role": "system", "content": system}]
        if self.summary:
            msgs.append({"role": "system", "content": f"Conversation summary memory:\n{self.summary}"})
        msgs.extend(self.window)
        return msgs


def init_elevenlabs() -> Optional[ElevenLabs]:
    if not ELEVENLABS_API_KEY:
        return None
    return ElevenLabs(api_key=ELEVENLABS_API_KEY)


def get_llm_client(api_key_override: Optional[str]) -> LLMClient:
    """Get or create LLM client based on provider."""
    if LLM_PROVIDER == "groq":
        key = (api_key_override or DEFAULT_GROQ_API_KEY).strip()
    else:
        key = (api_key_override or DEFAULT_OPENAI_API_KEY).strip()
    
    if not key:
        raise RuntimeError(f"Missing API key for {LLM_PROVIDER} LLM provider.")
    
    cache_key = f"{LLM_PROVIDER}:{key}"
    if cache_key not in llm_clients:
        llm_clients[cache_key] = LLMClient(key, LLM_PROVIDER)
    return llm_clients[cache_key]


def elevenlabs_tts_pcm(el: ElevenLabs, text: str) -> bytes:
    audio_gen = el.text_to_speech.convert(
        voice_id=ELEVENLABS_VOICE_ID,
        optimize_streaming_latency="2",
        output_format="pcm_16000",
        text=text,
        model_id="eleven_multilingual_v2",
    )
    pcm = bytearray()
    for chunk in audio_gen:
        if chunk:
            pcm.extend(chunk)
    if not pcm:
        raise RuntimeError("ElevenLabs returned empty audio")
    return bytes(pcm)


@dataclass
class SessionState:
    memory: Memory
    api_key: str
    last_lang: Optional[str] = None
    turns: int = 0
    lock: threading.Lock = field(default_factory=threading.Lock)


# =========================
# Initialize models at startup
# =========================

print("\n" + "="*60)
print("ðŸš€ Initializing Bilingual Voice Agent Server")
print("="*60)

whisper_model = load_whisper()
eleven_client = init_elevenlabs()
sessions: Dict[str, SessionState] = {}

print("="*60)
print("âœ… Server initialization complete")
print("="*60 + "\n")


# =========================
# API Endpoints
# =========================


def require_auth(x_auth: Optional[str] = Header(None)):
    if SERVER_AUTH_TOKEN and x_auth != SERVER_AUTH_TOKEN:
        raise HTTPException(status_code=401, detail="Invalid auth token")


def get_session(session_id: Optional[str], api_key_override: Optional[str]) -> tuple[str, SessionState]:
    llm_client = get_llm_client(api_key_override)
    if not session_id or session_id not in sessions:
        session_id = uuid.uuid4().hex
        sessions[session_id] = SessionState(memory=Memory(llm_client), api_key=llm_client.api_key)
    else:
        state = sessions[session_id]
        if state.api_key != llm_client.api_key:
            state.memory.llm = llm_client
            state.api_key = llm_client.api_key
    state = sessions[session_id]
    return session_id, state


@app.get("/healthz")
def healthz():
    return {"status": "ok", "llm_provider": LLM_PROVIDER, "llm_model": LLM_MODEL}


@app.post("/api/session")
def create_session(
    _: None = Depends(require_auth),
    x_api_key: Optional[str] = Header(None),
):
    """Create a new session. Accepts API key via x_api_key header."""
    try:
        session_id, _ = get_session(None, x_api_key)
    except RuntimeError as exc:
        raise HTTPException(status_code=500, detail=str(exc))
    return {"session_id": session_id}


@app.post("/api/process")
async def process_turn(
    audio: UploadFile = File(...),
    session_id: Optional[str] = Form(None),
    _: None = Depends(require_auth),
    x_api_key: Optional[str] = Header(None),
    api_key: Optional[str] = Form(None),
):
    """Process audio turn. Accepts API key via x_api_key header or api_key form field."""
    audio_bytes = await audio.read()
    if not audio_bytes:
        raise HTTPException(status_code=400, detail="Empty audio payload")

    api_key_override = api_key or x_api_key
    try:
        session_id, state = get_session(session_id, api_key_override)
    except RuntimeError as exc:
        raise HTTPException(status_code=500, detail=str(exc))

    wav_buf = io.BytesIO(audio_bytes)
    user_text, lang = whisper_transcribe(whisper_model, wav_buf)

    if not user_text:
        return {
            "session_id": session_id,
            "text": "",
            "lang": lang,
            "assistant_text": "",
            "tts_audio_b64": None,
            "tts_sample_rate": TARGET_SR,
        }

    with state.lock:
        state.memory.add_user(user_text)
        messages = state.memory.build_prompt(lang)
        assistant_text = state.memory.llm.complete(messages)
        state.memory.add_assistant(assistant_text)
        state.memory.maybe_summarize()
        state.last_lang = lang or state.last_lang
        state.turns += 1

    audio_b64 = None
    if eleven_client:
        try:
            pcm_bytes = elevenlabs_tts_pcm(eleven_client, assistant_text)
            audio_b64 = base64.b64encode(pcm_bytes).decode("ascii")
        except Exception as exc:
            print("TTS error:", exc)

    return {
        "session_id": session_id,
        "text": user_text,
        "lang": lang or state.last_lang,
        "assistant_text": assistant_text,
        "tts_audio_b64": audio_b64,
        "tts_sample_rate": TARGET_SR,
    }


if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "8000"))
    print(f"Starting server with LLM provider: {LLM_PROVIDER}")
    print(f"Using model: {LLM_MODEL}")
    uvicorn.run(app, host="0.0.0.0", port=port)